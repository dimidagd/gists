{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimidagd/gists/blob/main/Vision_transformers_patching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade yt-dlp\n",
        "!apt -y install ffmpeg lame\n",
        "!yt-dlp 'https://www.youtube.com/watch?v=CoL8Gtvxfl0' --download-sections \"*3-7\" -f mp4 -o './sample_video_dog.mp4'"
      ],
      "metadata": {
        "id": "ONkerRCRB6GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/seshuad/IMagenet\n",
        "! ls 'IMagenet/tiny-imagenet-200/train'\n",
        "! ls 'IMagenet/tiny-imagenet-200/train/n01443537/images' | wc -l"
      ],
      "metadata": {
        "id": "nxxCXG7NHYgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from itertools import repeat\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "n5BXCH3Bxxcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYcGO7lTJ9C8"
      },
      "outputs": [],
      "source": [
        "# Taken from https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/eval/segmentation_m2f/models/backbones/vit.py#L112\n",
        "\n",
        "\n",
        "def to_2tuple(x):\n",
        "    return tuple(repeat(x, 2))\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"2D Image to Patch Embedding.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True, bias=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.flatten = flatten\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        _, _, H, W = x.shape\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
        "        x = self.norm(x)\n",
        "        return x, H, W\n",
        "\n",
        "\n",
        "# Check PatchEmbedding layer. Essntially the image is both patched and convolved with the filter as the kernel is learnable.\n",
        "# Should result\n",
        "X = torch.rand(1,3,224,224)\n",
        "Z, H ,W = PatchEmbed(img_size=224, patch_size=16,embed_dim=1)(X)\n",
        "print(Z.shape,H,W)\n",
        "print(\"B 14*14 embed_dim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQe-mHB1KSPu"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "model_cards = {\n",
        "    's':{'name':'dinov2_vits14_reg','feat_dim':384},\n",
        "    'b':{'name':'dinov2_vitb14_reg','feat_dim':768},\n",
        "    'l':{'name':'dinov2_vitl14_reg','feat_dim':1024},\n",
        "    'g':{'name':'dinov2_vitg14_reg','feat_dim':1536},\n",
        "}\n",
        "model = \"s\"\n",
        "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', model_cards[model]['name'])\n",
        "dinov2_vitl14 = dinov2_vitl14.to(device)\n",
        "patch_size = dinov2_vitl14.patch_size # patchsize=14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFT4IcIPPJhl"
      },
      "outputs": [],
      "source": [
        "# Defines the output resolution\n",
        "n_patches = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssi2Ty_lL9Yu"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_patches_h, n_patches_w =  int(n_patches), int(n_patches)\n",
        "patch_h  = patch_size*n_patches_h//patch_size\n",
        "patch_w  = patch_h\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.Resize(2*patch_size*n_patches),\n",
        "                                transforms.CenterCrop(patch_size*n_patches), #should be multiple of model patch_size\n",
        "                                transforms.ToTensor(),\n",
        "                                ])\n",
        "\n",
        "#520//14\n",
        "\n",
        "feat_dim = model_cards[model]['feat_dim'] # vitl14\n",
        "\n",
        "class_id = \"n01629819\"\n",
        "folder_path = f'IMagenet/tiny-imagenet-200/train/{class_id}/images'\n",
        "total_features  = []\n",
        "n_frames = 4\n",
        "\n",
        "with torch.no_grad():\n",
        "  for img_path in tqdm(os.listdir(folder_path)[0:n_frames]):\n",
        "    img_path = os.path.join(folder_path, img_path)\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_t = transform(img).to(device)\n",
        "\n",
        "    features_dict = dinov2_vitl14.forward_features(img_t.unsqueeze(0))\n",
        "    features = features_dict['x_norm_patchtokens']\n",
        "    total_features.append(features)\n",
        "\n",
        "total_features = torch.cat(total_features, dim=0)\n",
        "total_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_dimensions(video_path):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    # Get the width and height of the video frames\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    return height, width\n"
      ],
      "metadata": {
        "id": "dlydtCBOEivV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "video_path = 'sample_video_dog.mp4'\n",
        "video_h, video_w = get_video_dimensions(video_path)\n",
        "n_patches_h, n_patches_w =  int(n_patches*video_h/video_w), int(n_patches)\n",
        "patch_h, patch_w  = patch_size*n_patches_h//patch_size, patch_size*n_patches_w//patch_size\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.Resize((int(patch_size*n_patches_h),int(patch_size*n_patches_w))),\n",
        "                                transforms.CenterCrop((patch_size*n_patches_h,patch_size*n_patches_w)), #should be multiple of model patch_size\n",
        "                                transforms.ToTensor(),\n",
        "                                ])\n",
        "n_frames = 20*4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to preprocess frames\n",
        "def preprocess_frame(frame):\n",
        "    # Resize the frame to match DINOv2 input size (224x224)\n",
        "    resized_frame = cv2.resize(frame, (224, 224))\n",
        "    # Convert the frame to RGB\n",
        "    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    rgb_frame = Image.fromarray(rgb_frame)\n",
        "    normalized_frame = transform(rgb_frame)\n",
        "    # Add batch dimension\n",
        "    normalized_frame = normalized_frame.unsqueeze(0)\n",
        "    return normalized_frame\n",
        "\n",
        "# Load the video\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Process each frame of the video\n",
        "frame_count = 0\n",
        "total_features  = []\n",
        "input_frames = []\n",
        "with tqdm(total=n_frames) as pbar:\n",
        "  while(cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "          img_t = preprocess_frame(frame)\n",
        "          img_t = img_t.to(device)\n",
        "          with torch.no_grad():\n",
        "            features_dict = dinov2_vitl14.forward_features(img_t)\n",
        "          features = features_dict['x_norm_patchtokens'].detach().cpu()\n",
        "          total_features.append(features)\n",
        "          input_frames.append(img_t.detach().cpu())\n",
        "          frame_count += 1\n",
        "          pbar.update(1)\n",
        "          if len(total_features)==n_frames:\n",
        "            break\n",
        "      else:\n",
        "          break\n",
        "\n",
        "total_features = torch.cat(total_features, dim=0)\n",
        "input_features = torch.cat(input_frames, dim=0)\n",
        "print(\"features shape\",total_features.shape)\n",
        "# Convertensort the tensor to a NumPy array\n",
        "img = img_t[-1].permute(1, 2, 0).cpu().numpy()\n",
        "from matplotlib import pyplot as plt\n",
        "# Plot the image\n",
        "print(\"image shape\",img.shape)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Turn off axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cux0U6dYchjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_features.shape"
      ],
      "metadata": {
        "id": "8hvEWeXl9ToG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHC4m91XPcTr"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First PCA to Seperate Background\n",
        "# sklearn expects 2d array for traning\n",
        "total_features = total_features.reshape(n_frames * patch_h*patch_w, feat_dim).detach().cpu() #4(*H*w, 1024)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(total_features)\n",
        "pca_features = pca.transform(total_features)\n",
        "\n",
        "# visualize PCA components for finding a proper threshold\n",
        "# 3 histograms for 3 components\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.hist(pca_features[:, 0])\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(pca_features[:, 1])\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(pca_features[:, 2])\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTEcZ9w6PyvF"
      },
      "outputs": [],
      "source": [
        "# min_max scale\n",
        "pca_features[:, 0] = (pca_features[:, 0] - pca_features[:, 0].min()) / \\\n",
        "                     (pca_features[:, 0].max() - pca_features[:, 0].min())\n",
        "# pca_features = sklearn.processing.minmax_scale(pca_features)\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(pca_features[i*patch_h*patch_w : (i+1)*patch_h*patch_w, 0].reshape(patch_h, patch_w))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "threshold = np.median(pca_features[:, 0])\n",
        "\n",
        "# segment/seperate the backgound and foreground using the first component\n",
        "pca_features_bg = pca_features[:, 0] < 1.5*threshold # from first histogram\n",
        "pca_features_fg = ~pca_features_bg\n",
        "\n",
        "# plot the pca_features_bg\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(pca_features_bg[i * patch_h * patch_w: (i+1) * patch_h * patch_w].reshape(patch_h, patch_w))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsSXwOlnT9j0"
      },
      "outputs": [],
      "source": [
        "# 2nd PCA for only foreground patches\n",
        "pca.fit(total_features[pca_features_fg])\n",
        "pca_features_left = pca.transform(total_features[pca_features_fg])\n",
        "\n",
        "for i in range(3):\n",
        "    # min_max scaling\n",
        "    pca_features_left[:, i] = (pca_features_left[:, i] - pca_features_left[:, i].min()) / (pca_features_left[:, i].max() - pca_features_left[:, i].min())\n",
        "\n",
        "pca_features_rgb = pca_features.copy()\n",
        "# for black background\n",
        "pca_features_rgb[pca_features_bg] = 0\n",
        "# new scaled foreground features\n",
        "pca_features_rgb[pca_features_fg] = pca_features_left\n",
        "\n",
        "# reshaping to numpy image format\n",
        "pca_features_rgb = pca_features_rgb.reshape(n_frames, patch_h, patch_w, 3)\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(pca_features_rgb[i])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation function to convert float tensor in [0, 1] to int8 tensor in [0, 255]\n",
        "def float_to_int8(image):\n",
        "    return (image * 255).permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "\n",
        "# Assuming 'tensor' is your torch tensor with dtype=float64 and values in the range [0, 1]\n",
        "# Shape of the tensor: [batch_size, channels, height, width]\n",
        "tensor = torch.rand(1, 3, 224, 224)  # Example tensor\n",
        "\n",
        "\n",
        "# Scale so that minimum edge is around 400px\n",
        "scale = 400/max(n_patches_w,n_patches_h)\n",
        "\n",
        "# Define the transform\n",
        "transform_to_int8 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((int(n_patches_h*scale), int(n_patches_w*scale)),interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: float_to_int8(x)),\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "HsQLAknEmdMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dino_frames = np.array([transform_to_int8(frame) for frame in pca_features_rgb])\n",
        "input_frames = np.array([transform_to_int8(frame) for frame in input_features])\n",
        "\n",
        "frames =  np.concatenate((input_frames,dino_frames),axis=2)"
      ],
      "metadata": {
        "id": "4fOxpxF9nA3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "# converted_images = np.clip(transform_to_int8(pca_features_rgb), 0, 255)\n",
        "imageio.mimsave('./animation.gif', frames, fps=10)\n"
      ],
      "metadata": {
        "id": "-vtn1TFxna2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as ImageDisplay\n",
        "ImageDisplay('./animation.gif')"
      ],
      "metadata": {
        "id": "-zxGa-I6UyJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AiENv0S5-LNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPhm/ChPPzm9KMo3FuwtsUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}