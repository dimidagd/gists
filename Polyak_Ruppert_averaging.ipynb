{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2PXZ+b56gwWIFEtEK6RNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimidagd/gists/blob/main/Polyak_Ruppert_averaging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polyak-Ruppert averaging\n",
        "\n",
        "\n",
        "\n",
        "> Polyak-Ruppert averaging is a technique used in optimization to compute a weighted average of iterates obtained during the optimization process. By averaging multiple parameter vectors, it can improve generalization, stability, and robustness to hyperparameter choices. This technique is widely used in machine learning and optimization research and is applicable to various optimization algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "WL667sje4e3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhILLBx84aDn",
        "outputId": "d3fee6df-dbd7-44e8-c84a-42ac0883f9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:03<00:00, 300.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between models -18.67245864868164\n",
            "loss with averaged parameters: 1.3458468914031982\n",
            "loss in last epoch: 1.3584779500961304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "# Define a simple linear regression model\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear1 = nn.Linear(1, 100)\n",
        "        self.linear2 = nn.Linear(100, 100)\n",
        "        self.linear3 = nn.Linear(100, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        return self.linear3(self.activation(self.linear2(self.activation(self.linear1(x)+x))))\n",
        "\n",
        "# Generate some dummy data\n",
        "torch.manual_seed(42)\n",
        "x = torch.randn(1000, 1)\n",
        "x_test = torch.randn(100, 1)\n",
        "F = lambda input_features: 3*input_features + input_features**2  + torch.randn(input_features.shape[0], 1) **2 # y = 3x + 2 + noise\n",
        "y = F(x)\n",
        "y_test = F(x_test)\n",
        "# Initialize model, optimizer, and Polyak-Ruppert averaging parameters\n",
        "model = LinearRegression()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "avg_params = [p.clone().detach() for p in model.parameters()]\n",
        "alpha = 0.91  # Decay rate for averaging\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = nn.functional.mse_loss(outputs, y)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Polyak-Ruppert averaging parameters\n",
        "    with torch.no_grad():\n",
        "        for p, avg_p in zip(model.parameters(), avg_params):\n",
        "            avg_p.mul_(alpha).add_(p, alpha=1 - alpha)\n",
        "y_test_pred_no_avg = model(x_test)\n",
        "# Use averaged parameters for evaluation\n",
        "with torch.no_grad():\n",
        "    # Set model parameters to the averaged parameters\n",
        "    for p, avg_p in zip(model.parameters(), avg_params):\n",
        "        p.copy_(avg_p)\n",
        "\n",
        "    # Evaluate the model on some test data\n",
        "    y_test_pred = model(x_test)\n",
        "    print(\"Difference between models\",(y_test_pred_no_avg - y_test_pred).sum().item())\n",
        "    criterion = nn.MSELoss()\n",
        "    loss = torch.sqrt(criterion(y_test_pred, y_test))\n",
        "    loss_no_avg = torch.sqrt(criterion(y_test_pred_no_avg, y_test))\n",
        "    print(\"loss with averaged parameters:\",loss.item())\n",
        "    print(\"loss in last epoch:\",loss_no_avg.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gibg1mjfKw6l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}